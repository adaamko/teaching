{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# AUTMI Seminar 2020/2021 Spring\n",
    "\n",
    "## Introduction to Machine Learning\n",
    "\n",
    "## February 25, 2021"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data science\n",
    "\n",
    "> The nontrivial extraction of implicit, previously unknown, and\n",
    "> potentially useful information from data.\n",
    "\n",
    "-   non-trivial\n",
    "\n",
    "-   relationship between data points\n",
    "\n",
    "-   (large) dataset\n",
    "\n",
    "-   make predictions on unknown examples\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Examples and counterexamples Convenience store statistics\n",
    "\n",
    "-   Number of customers\n",
    "\n",
    "    -   trivial information\n",
    "\n",
    "-   Last month’s income\n",
    "\n",
    "    -   trivial information\n",
    "\n",
    "-   Items most frequently bought together\n",
    "\n",
    "    -   *finding frequent itemsets*\n",
    "\n",
    "-   How many cashiers need to be open at Friday 16 pm?\n",
    "\n",
    "    -   customer queuing model\n",
    "\n",
    "    -   *time series modeling*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Representations\n",
    "\n",
    "To be able to run machine learning algorithms the computer needs numerical representations. For natural text input this means we need a mapping that converts strings to a numerical represenatation. **one-hot encoding** is the easiest approach where we map each word to an integer id."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Vector Representation\n",
    "\n",
    "\n",
    "- sample  \n",
    "    - one data point - <span style=\"color: darkred\">vector</span>\n",
    "\n",
    "- feature  \n",
    "    - a property or attribute of a sample - <span style=\"color: darkred\">one element of a vector</span>\n",
    "\n",
    "    - length of the mail\n",
    "\n",
    "    -   sender\n",
    "\n",
    "    -   Does it contain the word *Rolex*?\n",
    "\n",
    "    -   Does it contain the expression *Trust fund*?\n",
    "\n",
    "- dataset  \n",
    "    - collections of all samples - <span style=\"color: darkred\">matrix</span>\n",
    "\n",
    "- label  \n",
    "    - correct *answers* for all samples in a dataset - <spanstyle=\"color: darkred\">vector</span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Machine Learning and Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Neural networks and Deep learning\n",
    "- Neural network is inspired by the information processing methods of biological nervous systems\n",
    "- It is composed of neurons, each layer connected to the next\n",
    "- Deep learning is a neural network consisting of multiple layers\n",
    "    - the idea is not new\n",
    "    - it is returned because of the rise of the GPUs\n",
    "    - good frameworks (Pytorch, Tensorflow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Deep Learning\n",
    "\n",
    "- it has a black-box nature\n",
    "- interpreting them is hard\n",
    "- we don't exactly know the reasoning behind a decision\n",
    "- can we trust Deep learning?\n",
    "- the latest language model of **OpenAI**, **GPT-3**, has 175B trainable parameters [link](https://news.developer.nvidia.com/openai-presents-gpt-3-a-175-billion-parameters-language-model/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/dl/network.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "this is a __feed forward neural network__ with two hidden layers. Each neuron contains an activation function:\n",
    "\n",
    "$$\\mathbf{h_1} = \\sigma (\\mathbf{W_1 x})$$\n",
    "$$\\mathbf{h_2} = \\sigma (\\mathbf{W_2 h_1})$$\n",
    "$$\\mathbf{y} = \\sigma (\\mathbf{W_3 h_2})$$\n",
    "\n",
    "$\\sigma$: activation function, typically non-linear such as the sigmoid\n",
    "function $$\\sigma(x) = \\frac{1}{1 + e^{-x}}$$\n",
    "\n",
    "During the training, the weights are learned to predict a value of a new input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__What is inside a neuron?__\n",
    "\n",
    "![perceptron](https://c.mql5.com/2/35/artificialneuron__1.gif)\n",
    "\n",
    "\n",
    "*image from https://www.mql5.com/en/blogs/post/724245*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "!pip install transformers==4.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "text_generator = pipeline(\"text-generation\")\n",
    "print(text_generator(\"The grade for my homework will be\", max_length=50, do_sample=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Allocate a pipeline for question-answering\n",
    "question_answerer = pipeline('question-answering')\n",
    "question_answerer({\n",
    "    'question': 'Who went to the store ?',\n",
    "    'context': 'Adam went to the store yesterday.'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import Conversation\n",
    "\n",
    "chit_chat = pipeline('conversational')\n",
    "conversation_1 = Conversation(\"Hi! How are you?\")\n",
    "conversation_2 = Conversation(\"Do you like movies?\")\n",
    "conversation_3 = Conversation(\"What is my favourite movie?\")\n",
    "chit_chat([conversation_1, conversation_2, conversation_3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ML vs DL\n",
    "\n",
    "<img src=\"img/dl/ai_ml_dl.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## ML vs DL\n",
    "\n",
    "- Deep learning\n",
    "    - Automatic feature engineering\n",
    "    - Scalable with big data\n",
    "    - Can solve non-separable problems as well (traditional methods struggle with non-linearity)\n",
    "    - Currently most state-of-the-art methods are based on DL\n",
    "- Traditional Machine Learning\n",
    "    - Feature extraction is done manually\n",
    "    - Can learn relatively well from small data (DL can’t)\n",
    "    - Scalability is worse with big data\n",
    "    - It can be enough for small tasks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning types \n",
    "-   **Supervised learning**\n",
    "\n",
    "    -   is a problem where for every input variable(x) there is an ouput\n",
    "        variable(y) in the training data\n",
    "\n",
    "    -   the preparation of the output variables is usually done with\n",
    "        Human resources - Labeled data\n",
    "\n",
    "-   **Unsupervised learning**\n",
    "\n",
    "    -   is a problem where only the input variable(x) is present in the\n",
    "        training data\n",
    "\n",
    "    -   still can be very useful since labeling data is very resource\n",
    "        hungry and expensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Learning problems\n",
    "\n",
    "-   __Classification__ (supervised learning)\n",
    "\n",
    "    -   assign a label for each sample\n",
    "\n",
    "    -   labels are predefined and usually not very numerous\n",
    "\n",
    "    -   e.g. sentiment analysis\n",
    "\n",
    "-   __Regression__ (supervised learning)\n",
    "\n",
    "    -   predict a continuous variable\n",
    "\n",
    "    -   e.g. predict real estate prices, stock market based on history,\n",
    "        location, amenities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "-   __Clustering__ (unsupervised learning)\n",
    "\n",
    "    -   group samples into clusters according to a similarity measure\n",
    "     \n",
    "    -   e.g. group similar facebook comments\n",
    "    \n",
    "    -   goal: high intra-group similarity (samples in the same cluster\n",
    "        should be similar to each other), low inter-group similarity\n",
    "        (samples in different clusters shouldn’t be similar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/dl/sup_unsup.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluation - Binary classification\n",
    "\n",
    "<img src=\"img/dl/true_false.png\" />\n",
    "\n",
    "**Accuracy**: fraction of correctly guessed labels among all the samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### __Precision, recall and F-score:__\n",
    "\n",
    "**Precision**: fraction of positive samples among those labeled positive\n",
    "$$\\text{Precision}=\\frac{tp}{tp+fp}$$\n",
    "\n",
    "**Recall**: fraction of recovered positive samples of all positive\n",
    "samples $$\\text{Recall}=\\frac{tp}{tp+fn}$$\n",
    "\n",
    "**F-score**: harmonic mean of precision and recall\n",
    "$$\\text{F-score} = 2 * \\frac{\\text{prec}  \\text{rec}}{\\text{prec} + \\text{rec}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Evaluation - regression Root-mean-square error\n",
    "\n",
    "$$\\operatorname{RMSE}=\\sqrt{\\frac{\\sum_{t=1}^n (\\hat y_t - y_t)^2}{n}},$$\n",
    "\n",
    "where $\\hat y_t$ are the predicted values, $y_t$ is the true value and\n",
    "$n$ is the number of samples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Train, Validation and Test set\n",
    "\n",
    "- __training set__\n",
    "    - part of the dataset used for training -\n",
    "\n",
    "\n",
    "- __validation dataset__\n",
    "    - part of the dataset used for cross-validation, early stopping and\n",
    "    hyperparameter tuning\n",
    "\n",
    "\n",
    "- __test set__\n",
    "    - part of the dataset used for testing trained models. Your method\n",
    "    should only be tested once on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"img/dl/train_test_val.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Terminology\n",
    "\n",
    "-   How do ML/DL algorithms learn?\n",
    "\n",
    "-   **Loss function**: helps to calculate the prediction loss of our\n",
    "    network, which tells us how bad/good is our model.\n",
    "\n",
    "-   We want to **optimize** the loss/cost function.\n",
    "\n",
    "-   How?\n",
    "\n",
    "    -   **Gradient descent** helps us find the global minima of the loss\n",
    "        function\n",
    "\n",
    "    -   **Backpropagation** algorithm is used to propagate the error\n",
    "        back to the weights of the model and updates them"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Important concepts in Machine Learning\n",
    "\n",
    "- __Cost Function__: used to measure how badly our models are performing on a data\n",
    "- __Parameters__: variables that are updated during the training\n",
    "- __Sample__: single row in our data\n",
    "- __Batch size__: the number of samples our model works throught before updating the weights\n",
    "- __Epoch__: one epoch means that each sample in the training dataset was iterated through the model\n",
    "- __Iteration__ – one update on the weights. It happens once for each batch.\n",
    "- __Hyperparameters__ – variables that don't change during training (number of epochs, batch size, learning rate, etc..)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- __Gradient descent__: used to find the global optimum in the cost function\n",
    "\n",
    "<img src=\"img/dl/gradient.gif?raw=true\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Over- and Underfitting?\n",
    "![under_over](https://miro.medium.com/max/2400/1*JZbxrdzabrT33Yl-LrmShw.png)\n",
    "*image from https://miro.medium.com/max/2400/1*JZbxrdzabrT33Yl-LrmShw.png*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent neural networks\n",
    "\n",
    "In NLP, recurrent neural networks (RNN) are commonly used to analyse sequences. It takes in a sequence of words, one at a time, and produces hidden states ($h$) after each steps. RNN-s are used recurrently by feeding in the current word and the hidden state from the previous word.\n",
    "\n",
    "Once we have our final hidden state, $h_T$, (from feeding in the last word in the sequence, $x_T$) we feed it through a linear layer, $f$ (fully connected layer) to reduce the dimension into the dimension of the labels."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![rnn](https://github.com/bentrevett/pytorch-sentiment-analysis/raw/79bb86abc9e89951a5f8c4a25ca5de6a491a4f5d/assets/sentiment1.png)\n",
    "\n",
    "_(image from bentrevett)_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![rnn2](https://miro.medium.com/max/1400/1*WMnFSJHzOloFlJHU6fVN-g.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![rnn3](https://miro.medium.com/max/770/1*o-Cq5U8-tfa1_ve2Pf3nfg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTM\n",
    "\n",
    "One of the biggest problem of recurrent neural networks is the vanishing gradient problem. It happens when the gradient shrinks during bakcpropagarion. If it becomes very small, the network stops learning. This mostly happen when long sentences are present. LSTM networks address this problem by having an inner memory cell to remember important information or forget others. LSTM has a similar flow as a RNN, it processes data and passes information as it propagates forward. The difference is in the operations within the cells."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![lstm](https://miro.medium.com/max/770/1*0f8r3Vd-i4ueYND1CUrhMA.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "__LSTM__ consists of:\n",
    "\n",
    "- __Forget gate__\n",
    "    - Decides what information should be kept or thrown away\n",
    "    - Information from the previous hidden state and from the current input\n",
    "\n",
    "![forget](https://miro.medium.com/max/770/1*GjehOa513_BgpDDP6Vkw2Q.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- __Input gate__\n",
    "    - Decides what information is relevant to add from the current step\n",
    "\n",
    "![input](https://miro.medium.com/max/770/1*TTmYy7Sy8uUXxUXfzmoKbA.gif)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- __Cell state__\n",
    "\n",
    "![cell](https://miro.medium.com/max/770/1*S0rXIeO_VoUVOyrYHckUWg.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- __Output gate__\n",
    "    - Determines what the next hidden state should be\n",
    "\n",
    "![lstm2](https://miro.medium.com/max/770/1*VOXRGhOShoWWks6ouoDN3Q.gif)\n",
    "\n",
    "_(images from [link](https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21))_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Building a classficiation pipeline\n",
    "\n",
    "The AG's news topic classification dataset is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the dataset above. It is used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\n",
    "\n",
    "The AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "!pip install torchtext==0.4\n",
    "!pip install torch\n",
    "!pip install pandas\n",
    "!pip install gensim\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "First we are going to download the dataset using [torchtext](https://pytorch.org/text/stable/index.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "NGRAMS = 2\n",
    "from torchtext import data\n",
    "from torchtext.datasets import text_classification\n",
    "import os\n",
    "if not os.path.isdir('./data'):\n",
    "    os.mkdir('./data')\n",
    "text_classification.DATASETS['AG_NEWS'](\n",
    "    root='./data', ngrams=NGRAMS, vocab=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Import the needed libraries\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now we use [pandas](https://pandas.pydata.org/) to read in the dataset into a DataFrame. We are also going to just take a fraction of the dataset to be more efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#1-World, 2-Sports, 3-Business, 4-Sci/Tech\n",
    "\n",
    "train_data = pd.read_csv(\"./data/ag_news_csv/train.csv\",quotechar='\"', names=['label', 'title', 'description'])\n",
    "test_data = pd.read_csv(\"./data/ag_news_csv/test.csv\",quotechar='\"', names=['label', 'title', 'description'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#1-World, 2-Sports, 3-Business, 4-Sci/Tech\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "train_data = train_data.groupby('label').apply(lambda x: x.sample(frac=0.2, random_state=1234)).sample(frac=1.0)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We need a way of converting raw data to features!\n",
    "![features](https://developers.google.com/machine-learning/crash-course/images/RawDataToFeatureVector.svg)\n",
    "\n",
    "_(image from [link](https://developers.google.com/machine-learning))_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The easiest way of converting raw data to features is called the [Bag of Words](https://en.wikipedia.org/wiki/Bag-of-words_model) model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "word_to_ix = defaultdict(int)\n",
    "for sent in train_data.title:\n",
    "    for word in sent.split():\n",
    "        if word not in word_to_ix:\n",
    "            word_to_ix[word] = len(word_to_ix)\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We are going to use Python's machine learning library, called [scikit-learn](https://scikit-learn.org/stable/) to build a classical ML pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "corpora = ['hello my name is adam','i am the instructor for this class']\n",
    "# instantiate the vectorizer object\n",
    "vectorizer = CountVectorizer()\n",
    "# convert th documents into a matrix\n",
    "wm = vectorizer.fit_transform(corpora)\n",
    "#retrieve the terms found in the corpora\n",
    "tokens = vectorizer.get_feature_names()\n",
    "df_vect = pd.DataFrame(data = wm.toarray(),index = ['Doc1','Doc1'],columns = tokens)\n",
    "df_vect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "\n",
    "X = vectorizer.fit(train_data.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "X.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "c = X.transform([\"Hello my name is adam\"]).toarray()\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We first build a __featurizer__ that takes raw texts as input and runs builds features on the whole dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import gensim\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split as split\n",
    "import numpy as np\n",
    "\n",
    "def vectorize_to_bow(tr_data, tst_data):\n",
    "    \n",
    "    tr_vectors = X.transform(tr_data)\n",
    "    \n",
    "    tst_vectors = X.transform(tst_data)\n",
    "    return tr_vectors, tst_vectors\n",
    "\n",
    "def get_features_and_labels(data, labels):\n",
    "    tr_data,tst_data,tr_labels,tst_labels = split(data,labels, test_size=0.3, random_state=1234)\n",
    "    \n",
    "    tst_vecs = []\n",
    "    tr_vecs = []\n",
    "    tr_vecs, tst_vecs = vectorize_to_bow(tr_data, tst_data)    \n",
    "    return tr_vecs, tr_labels, tst_vecs, tst_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tr_vecs, tr_labels, tst_vecs, tst_labels = get_features_and_labels(train_data.title, train_data.label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tr_vecs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "#Import a bunch of stuff from sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "rf  =  RandomForestClassifier(n_estimators=100, verbose=True, n_jobs=-1)\n",
    "svc = SVC()\n",
    "lr  = LogisticRegression(n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "#And then we run the training of the algorithms with the provided vectors and the labels\n",
    "rf.fit(tr_vecs, tr_labels)\n",
    "svc.fit(tr_vecs, tr_labels)\n",
    "lr.fit(tr_vecs, tr_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import eli5\n",
    "eli5.show_weights(lr, feature_names=X.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "After training we can run the trained models on the test dataset to get the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(type(tst_vecs))\n",
    "rf_pred = rf.predict(tst_vecs)\n",
    "svc_pred = svc.predict(tst_vecs)\n",
    "lr_pred = lr.predict(tst_vecs)\n",
    "print(\"Random Forest Test accuracy : {}\".format(accuracy_score(tst_labels, rf_pred)))\n",
    "print(\"SVC Test accuracy : {}\".format(accuracy_score(tst_labels, svc_pred)))\n",
    "print(\"Logistic Regression Test accuracy : {}\".format(accuracy_score(tst_labels, lr_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Bag of words are the simplest method for featurizing your data. If we want a more sophisticated method, we could use [TF-IDf](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- __TF__: The term frequency of a word in a document. \n",
    "- __IDF__: The inverse document frequency of the word across a set of documents. This means, how common or rare a word is in the entire document set. The closer it is to 0, the more common a word is.\n",
    "- The higher the score, the more relevant that word is in that particular document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![tfidf](https://miro.medium.com/max/700/1*qQgnyPLDIkUmeZKN2_ZWbQ.png)\n",
    "\n",
    "_(image from [link](https://miro.medium.com))_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=1000, use_idf=True)\n",
    "vectors = vectorizer.fit(train_data.title)\n",
    "\n",
    "tfidf_vectorizer_vectors = vectors.transform(train_data.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "first_vector_tfidfvectorizer=tfidf_vectorizer_vectors[4] \n",
    " \n",
    "# place tf-idf values in a pandas data frame \n",
    "df = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=vectors.get_feature_names(), columns=[\"tfidf\"]) \n",
    "df = df.sort_values(by=[\"tfidf\"],ascending=False)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Sklearn allows us to build [pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) with defining each step of the pipeline, like:\n",
    "- Vectorizers\n",
    "- Classifiers\n",
    "- Voting strategies\n",
    "- Optionally merge feature extraction from multiple sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from time import time\n",
    "\n",
    "eclf = VotingClassifier(estimators=[('lr', lr), ('svc', svc), ('rf', rf)], voting='hard')\n",
    "\n",
    "tr_data, tst_data, tr_labels, tst_labels = split(train_data.title, train_data.label, test_size=0.3, random_state=1234)\n",
    "\n",
    "for clf, label in zip([lr, svc, rf, eclf], ['Logistic Regression', 'Linear SVC', 'Random Forest', 'Ensemble']):\n",
    "    checker_pipeline = Pipeline([\n",
    "            ('vectorizer', TfidfVectorizer(max_features=10000,ngram_range=(1, 3))),\n",
    "            ('classifier', clf)\n",
    "        ])\n",
    "    print(\"Validation result for {}\".format(label))\n",
    "    checker_pipeline.fit(tr_data, tr_labels)\n",
    "\n",
    "    tst_pred = checker_pipeline.predict(tst_data)\n",
    "    print(\"{} Test accuracy : {}\".format(label, accuracy_score(tst_labels, tst_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Deep learning model using [Pytorch](https://pytorch.org/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "SEED = 1234\n",
    "\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features=10000)\n",
    "\n",
    "word_to_ix = vectorizer.fit(train_data.title)\n",
    "\n",
    "VOCAB_SIZE = len(word_to_ix.vocabulary_)\n",
    "NUM_LABELS = 4 \n",
    "\n",
    "\n",
    "tr_data, val_data = split(train_data, test_size=0.3, random_state=1234)\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tr_data_vecs = torch.FloatTensor(word_to_ix.transform(tr_data.title).toarray())\n",
    "tr_labels = tr_data.label.tolist()\n",
    "\n",
    "val_data_vecs = torch.FloatTensor(word_to_ix.transform(val_data.title).toarray())\n",
    "val_labels = val_data.label.tolist()\n",
    "\n",
    "tr_data_loader = [(sample, label-1) for sample, label in zip(tr_data_vecs, tr_labels)]\n",
    "val_data_loader = [(sample, label-1) for sample, label in zip(val_data_vecs, val_labels)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "train_iterator = DataLoader(tr_data_loader,\n",
    "                            batch_size=BATCH_SIZE,\n",
    "                            shuffle=True,\n",
    "                            )\n",
    "\n",
    "valid_iterator = DataLoader(val_data_loader,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          shuffle=False,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class BoWClassifier(nn.Module):  # inheriting from nn.Module!\n",
    "\n",
    "    def __init__(self, num_labels, vocab_size):\n",
    "        # calls the init function of nn.Module.  Dont get confused by syntax,\n",
    "        # just always do it in an nn.Module\n",
    "        super(BoWClassifier, self).__init__()\n",
    "\n",
    "        # Define the parameters that you will need.  In this case, we need A and b,\n",
    "        # the parameters of the affine mapping.\n",
    "        # Torch defines nn.Linear(), which provides the affine map.\n",
    "        self.linear = nn.Linear(vocab_size, num_labels)\n",
    "\n",
    "    def forward(self, bow_vec):\n",
    "        # Pass the input through the linear layer,\n",
    "        # then pass that through log_softmax.\n",
    "        # Many non-linearities and other functions are in torch.nn.functional\n",
    "        return F.log_softmax(self.linear(bow_vec), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "INPUT_DIM = VOCAB_SIZE\n",
    "OUTPUT_DIM = 4\n",
    "\n",
    "model = BoWClassifier(OUTPUT_DIM, INPUT_DIM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model = model.to(device)\n",
    "criterion = criterion.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![NLL](https://ljvmiranda921.github.io/assets/png/cs231n-ann/neg_log_demo.png)\n",
    "\n",
    "_(image from [link](https://ljvmiranda921.github.io))_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "def class_accuracy(preds, y):\n",
    "    \"\"\"\n",
    "    Returns accuracy per batch\n",
    "    \"\"\"\n",
    "    rounded_preds = preds.argmax(1)\n",
    "    correct = (rounded_preds == y).float() #convert into float for division \n",
    "    #target_names = ['class 0', 'class 1', 'class 2', 'class 3']\n",
    "    #print(classification_report(rounded_preds.cpu().numpy(), y.cpu().numpy(), target_names=target_names))\n",
    "    acc = correct.sum() / len(correct)\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "import torch.nn.functional as F\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    for texts, labels in iterator:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        predictions = model(texts)\n",
    "\n",
    "        loss = criterion(predictions, labels)\n",
    "        \n",
    "        acc = class_accuracy(predictions, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "    \n",
    "        for texts, labels in iterator:\n",
    "            predictions = model(texts)\n",
    "            loss = criterion(predictions, labels)\n",
    "            \n",
    "            acc = class_accuracy(predictions, labels)\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "N_EPOCHS = 50\n",
    "\n",
    "best_valid_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)\n",
    "    \n",
    "    end_time = time.time()\n",
    "\n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'tut1-model.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
